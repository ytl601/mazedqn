# è¡¥å……æ ¸å¿ƒå¯¼å…¥ï¼ˆå…³é”®ä¿®å¤ï¼‰
import torch
import torch.nn as nn
import torch.optim as optim
import random
import math

# ====================== 1. ä¼˜å…ˆç»éªŒå›æ”¾ï¼ˆPERï¼‰ ======================
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6, beta=0.4, device="cpu"):
        self.capacity = capacity
        self.alpha = alpha
        self.beta = beta
        self.buffer = []
        self.priorities = torch.zeros((capacity,), dtype=torch.float32)
        self.pos = 0
        self.device = device

    def add(self, experience):
        max_prio = self.priorities[:len(self.buffer)].max().item() if self.buffer else 1.0
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
        else:
            self.buffer[self.pos] = experience
        self.priorities[self.pos] = max_prio
        self.pos = (self.pos + 1) % self.capacity

    def sample(self, batch_size):
        if len(self.buffer) == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:len(self.buffer)]
        
        probs = prios ** self.alpha
        probs = probs / probs.sum()
        
        indices = torch.multinomial(probs, batch_size, replacement=False)
        samples = [self.buffer[idx.item()] for idx in indices]
        
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta)
        weights = weights / weights.max()
        weights = weights.to(self.device)
        
        states = torch.tensor([s[0] for s in samples], dtype=torch.float32).to(self.device)
        actions = torch.tensor([s[1] for s in samples], dtype=torch.long).to(self.device)
        rewards = torch.tensor([s[2] for s in samples], dtype=torch.float32).to(self.device)
        next_states = torch.tensor([s[3] for s in samples], dtype=torch.float32).to(self.device)
        dones = torch.tensor([s[4] for s in samples], dtype=torch.float32).to(self.device)
        
        return (states, actions, rewards, next_states, dones), indices, weights

    def update_priorities(self, indices, priorities):
        for idx, prio in zip(indices, priorities):
            self.priorities[idx.item()] = prio

    def __len__(self):
        return len(self.buffer)

# ====================== 2. DQNæ¨¡å‹ ======================
class DQN(nn.Module):
    def __init__(self, state_dim=4, action_dim=4, hidden_dim=64):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        self.relu = nn.ReLU()
        self.clamp_min = -20.0
        self.clamp_max = 20.0

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return torch.clamp(x, self.clamp_min, self.clamp_max)

# ====================== 3. è¿·å®«ç¯å¢ƒ ======================
class WalkerEnv:
    def __init__(self, grid_size=5, target_pos=(4, 4), start_pos=(0, 0), render=False):
        self.grid_size = grid_size
        self.start_pos = start_pos
        self.target_pos = target_pos
        self.current_pos = start_pos
        self.step_count = 0
        self.max_steps = 80
        self.action_space = 4  # 0:ä¸Š,1:ä¸‹,2:å·¦,3:å³
        self.action_map = {0: (0, -1), 1: (0, 1), 2: (-1, 0), 3: (1, 0)}
        self.state_dim = 4
        self.obstacles = {(1,1), (2,1), (4,1), (1,2), (4,2), (1,3)}
        
        # æ¸²æŸ“é…ç½®
        self.render_flag = render
        self.pygame = None
        if self.render_flag:
            try:
                import pygame
                self.pygame = pygame
                self.pygame.init()
                self.screen_size = 400
                self.cell_size = self.screen_size // self.grid_size
                self.screen = self.pygame.display.set_mode((self.screen_size, self.screen_size))
                self.pygame.display.set_caption("5x5 Maze DQN - Validation")
                self.clock = self.pygame.time.Clock()
            except ImportError:
                print("âš ï¸ Pygameæœªå®‰è£…ï¼Œç¦ç”¨æ¸²æŸ“")
                self.render_flag = False
        
        # è½¨è¿¹è·Ÿè¸ª
        self.visited = set()
        self.last_pos = None
        self.repeat_count = 0
        self.min_dist_to_target = self._calc_dist(self.current_pos, self.target_pos)

    def _calc_dist(self, pos1, pos2):
        """æ›¼å“ˆé¡¿è·ç¦»"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])

    def reset(self):
        self.current_pos = self.start_pos
        self.step_count = 0
        self.visited = {self.start_pos}
        self.last_pos = None
        self.repeat_count = 0
        self.min_dist_to_target = self._calc_dist(self.current_pos, self.target_pos)
        return self._get_state()

    def _get_state(self):
        x, y = self.current_pos
        tx, ty = self.target_pos
        
        # å½’ä¸€åŒ–åæ ‡
        norm_x = x / (self.grid_size - 1)
        norm_y = y / (self.grid_size - 1)
        
        # ç»ˆç‚¹æ–¹å‘ï¼ˆæ›¼å“ˆé¡¿è·ç¦»å½’ä¸€åŒ–ï¼‰
        dx = tx - x
        dy = ty - y
        dist = self._calc_dist((x,y), (tx,ty)) + 1e-6
        dir_x = dx / dist
        
        # æœ€è¿‘éšœç¢è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼‰
        min_obs_dist = self.grid_size
        for (ox, oy) in self.obstacles:
            obs_dist = self._calc_dist((x,y), (ox,oy))
            if obs_dist < min_obs_dist:
                min_obs_dist = obs_dist
        norm_obs_dist = min_obs_dist / self.grid_size
        
        return [norm_x, norm_y, dir_x, norm_obs_dist]

    def step(self, action):
        self.step_count += 1
        x, y = self.current_pos
        dx, dy = self.action_map[action]
        new_x = x + dx
        new_y = y + dy
        done = False
        info = {}

        # è¾¹ç•Œ/éšœç¢æ£€æµ‹
        valid_move = True
        if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:
            valid_move = False
        if (new_x, new_y) in self.obstacles:
            valid_move = False

        # æ›´æ–°ä½ç½®
        old_pos = self.current_pos
        if valid_move:
            self.current_pos = (new_x, new_y)
        else:
            self.current_pos = (x, y)

        # å¥–åŠ±ç³»ç»Ÿ
        reward = 0.0
        reward -= 0.1  # æ­¥æ•°æƒ©ç½š
        if not valid_move:
            reward -= 2.0  # æ’å¢™é‡ç½š
            self.repeat_count += 1
        
        # é‡å¤è®¿é—®æƒ©ç½š
        if self.current_pos in self.visited:
            reward -= 1.5
            self.repeat_count += 1
        else:
            reward += 0.5
            self.visited.add(self.current_pos)
            self.repeat_count = 0
        
        # è¿›åº¦å¥–åŠ±
        current_dist = self._calc_dist(self.current_pos, self.target_pos)
        if current_dist < self.min_dist_to_target:
            reward += 3.0
            self.min_dist_to_target = current_dist
        elif current_dist > self.min_dist_to_target:
            reward -= 1.0
        
        # ç»ˆç‚¹å¥–åŠ±
        if self.current_pos == self.target_pos:
            reward += 50.0
            done = True
            info["success"] = True
        
        # è¶…æ—¶/å¡å£³æƒ©ç½š
        if self.step_count >= self.max_steps or self.repeat_count > 5:
            done = True
            info["success"] = False
            reward -= 10.0

        return self._get_state(), reward, done, info

    def get_action_mask(self):
        """åŠ¨ä½œæ©ç ï¼š1=æœ‰æ•ˆï¼Œ0=æ— æ•ˆ"""
        x, y = self.current_pos
        mask = [1]*4
        for action in range(4):
            dx, dy = self.action_map[action]
            nx = x + dx
            ny = y + dy
            if nx < 0 or nx >= self.grid_size or ny < 0 or ny >= self.grid_size:
                mask[action] = 0
            if (nx, ny) in self.obstacles:
                mask[action] = 0
        return mask

    def render(self):
        if not self.render_flag or self.pygame is None:
            return
        
        # å¤„ç†é€€å‡ºäº‹ä»¶
        for event in self.pygame.event.get():
            if event.type == self.pygame.QUIT:
                self.close()
                exit()
        
        # ç»˜åˆ¶ç•Œé¢
        self.screen.fill((255, 255, 255))
        for i in range(self.grid_size + 1):
            self.pygame.draw.line(self.screen, (0,0,0), (i*self.cell_size, 0), (i*self.cell_size, self.screen_size), 1)
            self.pygame.draw.line(self.screen, (0,0,0), (0, i*self.cell_size), (self.screen_size, i*self.cell_size), 1)
        
        # ç»˜åˆ¶éšœç¢
        for (x, y) in self.obstacles:
            self.pygame.draw.rect(self.screen, (100, 100, 100), (x*self.cell_size + 2, y*self.cell_size + 2, self.cell_size - 4, self.cell_size - 4))
        
        # ç»˜åˆ¶èµ·ç‚¹/ç»ˆç‚¹
        self.pygame.draw.rect(self.screen, (0, 255, 0), (self.start_pos[0]*self.cell_size + 2, self.start_pos[1]*self.cell_size + 2, self.cell_size - 4, self.cell_size - 4))
        self.pygame.draw.rect(self.screen, (255, 0, 0), (self.target_pos[0]*self.cell_size + 2, self.target_pos[1]*self.cell_size + 2, self.cell_size - 4, self.cell_size - 4))
        
        # ç»˜åˆ¶å°çƒ
        cx = self.current_pos[0] * self.cell_size + self.cell_size // 2
        cy = self.current_pos[1] * self.cell_size + self.cell_size // 2
        self.pygame.draw.circle(self.screen, (0, 0, 255), (cx, cy), self.cell_size // 3)
        
        # æ›´æ–°ç”»é¢
        self.pygame.display.update()
        self.clock.tick(60)

    def close(self):
        if self.render_flag and self.pygame is not None:
            self.pygame.quit()

# ====================== 4. DQNæ™ºèƒ½ä½“ ======================
class DQNAgent:
    def __init__(self):
        self.env = WalkerEnv(render=True)
        self.state_dim = self.env.state_dim
        self.action_dim = self.env.action_space
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"å½“å‰è®­ç»ƒè®¾å¤‡ï¼š{self.device}")

        # æ¨¡å‹åˆå§‹åŒ–
        self.policy_net = DQN(self.state_dim, self.action_dim).to(self.device)
        self.target_net = DQN(self.state_dim, self.action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3)
        self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.95)
        self.loss_fn = nn.SmoothL1Loss(reduction='none')

        # ä¼˜å…ˆç»éªŒå›æ”¾
        self.memory = PrioritizedReplayBuffer(capacity=20000, alpha=0.6, beta=0.4, device=self.device)
        self.batch_size = 64

        # æ¢ç´¢ç­–ç•¥
        self.epsilon_start = 0.8
        self.epsilon_end = 0.05
        self.epsilon_decay = 500
        self.steps_done = 0

        # æŠ˜æ‰£å› å­
        self.gamma = 0.95

    def get_epsilon(self):
        """çº¿æ€§è¡°å‡æ¢ç´¢ç‡"""
        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \
                  math.exp(-1. * self.steps_done / self.epsilon_decay)
        self.steps_done += 1
        return max(self.epsilon_end, epsilon)

    def choose_action(self, state):
        """çº¯è´ªå¿ƒé€‰æ‹©ï¼ˆéªŒè¯æ—¶ç¦ç”¨æ¢ç´¢ï¼‰"""
        epsilon = self.get_epsilon()
        
        if random.random() < epsilon:
            mask = self.env.get_action_mask()
            valid_actions = [i for i, m in enumerate(mask) if m == 1]
            return random.choice(valid_actions)
        
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)
            q_vals = self.policy_net(state_tensor)
            
            # åŠ¨ä½œæ©ç 
            mask = torch.tensor(self.env.get_action_mask(), dtype=torch.float32).to(self.device)
            q_vals = q_vals * mask - 1e9 * (1 - mask)
            
            action = q_vals.argmax().item()
        return action

    def learn(self):
        if len(self.memory) < self.batch_size:
            return 0.0

        # é‡‡æ ·ç»éªŒ
        (states, actions, rewards, next_states, dones), indices, weights = self.memory.sample(self.batch_size)

        # è®¡ç®—å½“å‰Qå€¼
        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # Double DQNç›®æ ‡Qå€¼
        with torch.no_grad():
            next_actions = self.policy_net(next_states).argmax(1)
            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + self.gamma * next_q * (1 - dones)

        # è®¡ç®—æŸå¤±
        loss = self.loss_fn(current_q, target_q)
        loss = (loss * weights).mean()

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)
        self.optimizer.step()

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.steps_done % 50 == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        # æ›´æ–°ä¼˜å…ˆçº§
        td_error = torch.abs(current_q - target_q).detach().cpu()
        self.memory.update_priorities(indices, td_error)

        # å­¦ä¹ ç‡è¡°å‡
        self.lr_scheduler.step()

        return loss.item()

    def train(self, episodes=500):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        for ep in range(episodes):
            state = self.env.reset()
            total_reward = 0.0
            loss_sum = 0.0
            done = False

            while not done:
                action = self.choose_action(state)
                next_state, reward, done, info = self.env.step(action)
                total_reward += reward

                # å­˜å‚¨ç»éªŒ
                self.memory.add((state, action, reward, next_state, done))

                # å­¦ä¹ 
                loss = self.learn()
                loss_sum += loss

                # æ›´æ–°çŠ¶æ€
                state = next_state
                self.env.render()

            # æ‰“å°è®­ç»ƒæ—¥å¿—
            avg_loss = loss_sum / self.env.step_count if self.env.step_count > 0 else 0.0
            success = info.get("success", False)
            epsilon = self.get_epsilon()
            print(f"Train Episode {ep+1:4d} | Reward: {total_reward:6.1f} | Loss: {avg_loss:.4f} | Epsilon: {epsilon:.3f} | Success: {success}")

        self.env.close()

# ====================== 5. éªŒè¯æ ¸å¿ƒé€»è¾‘ ======================
def validate_agent(model_path=None, episodes=100, render=True):
    """
    éªŒè¯è®­ç»ƒåçš„æ™ºèƒ½ä½“
    :param model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
    :param episodes: éªŒè¯è½®æ•°
    :param render: æ˜¯å¦å¯è§†åŒ–
    """
    # åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹
    env = WalkerEnv(render=render)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = DQN(env.state_dim, env.action_space).to(device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    if model_path:
        try:
            model.load_state_dict(torch.load(model_path, map_location=device))
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹ï¼š{model_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{e} | ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
    else:
        print("âš ï¸ æœªæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹ï¼ˆä»…ä½œå¯¹æ¯”ï¼‰")
    
    model.eval()  # è¯„ä¼°æ¨¡å¼

    # åˆå§‹åŒ–éªŒè¯æŒ‡æ ‡
    total_success = 0
    total_steps = 0
    total_reward = 0
    success_episodes = []
    fail_episodes = []

    # å¼€å§‹éªŒè¯
    print("\n========== å¼€å§‹éªŒè¯ ==========")
    print(f"éªŒè¯è½®æ•°ï¼š{episodes} | æ¸²æŸ“ï¼š{render} | è®¾å¤‡ï¼š{device}")
    for ep in range(episodes):
        state = env.reset()
        episode_reward = 0.0
        episode_steps = 0
        done = False
        path = [env.current_pos]

        while not done:
            # çº¯è´ªå¿ƒé€‰æ‹©ï¼ˆç¦ç”¨æ¢ç´¢ï¼‰
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                q_vals = model(state_tensor)
                mask = torch.tensor(env.get_action_mask(), dtype=torch.float32).to(device)
                q_vals = q_vals * mask - 1e9 * (1 - mask)
                action = q_vals.argmax().item()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            episode_reward += reward
            episode_steps += 1
            path.append(env.current_pos)
            
            # å¯è§†åŒ–
            env.render()
            
            # æ›´æ–°çŠ¶æ€
            state = next_state

        # ç»Ÿè®¡æŒ‡æ ‡
        total_success += 1 if info["success"] else 0
        total_steps += episode_steps
        total_reward += episode_reward

        if info["success"]:
            success_episodes.append(ep+1)
            print(f"Val Episode {ep+1:4d} | æˆåŠŸ | æ­¥æ•°ï¼š{episode_steps:3d} | å¥–åŠ±ï¼š{episode_reward:6.1f}")
        else:
            fail_episodes.append(ep+1)
            print(f"Val Episode {ep+1:4d} | å¤±è´¥ | æ­¥æ•°ï¼š{episode_steps:3d} | å¥–åŠ±ï¼š{episode_reward:6.1f} | æœ€åä½ç½®ï¼š{env.current_pos}")

    # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
    avg_steps = total_steps / episodes
    avg_reward = total_reward / episodes
    success_rate = total_success / episodes * 100

    # è¾“å‡ºéªŒè¯æŠ¥å‘Š
    print("\n========== éªŒè¯æŠ¥å‘Š ==========")
    print(f"æ€»éªŒè¯è½®æ•°ï¼š{episodes}")
    print(f"æˆåŠŸè½®æ•°ï¼š{total_success} | å¤±è´¥è½®æ•°ï¼š{episodes - total_success}")
    print(f"æˆåŠŸç‡ï¼š{success_rate:.2f}%")
    print(f"å¹³å‡æ­¥æ•°ï¼š{avg_steps:.2f} | å¹³å‡å¥–åŠ±ï¼š{avg_reward:.2f}")
    
    # æ•ˆæœåˆ¤æ–­
    print("\n========== æ•ˆæœåˆ¤æ–­ ==========")
    if success_rate >= 90:
        print("âœ… ä¼˜ç§€ï¼šæˆåŠŸç‡â‰¥90%ï¼Œç®—æ³•ç¨³å®šæ”¶æ•›")
    elif success_rate >= 70:
        print("âš ï¸ è‰¯å¥½ï¼šæˆåŠŸç‡70%-90%ï¼Œéœ€å°‘é‡è°ƒä¼˜")
    elif success_rate >= 50:
        print("âš ï¸ ä¸€èˆ¬ï¼šæˆåŠŸç‡50%-70%ï¼Œéœ€ä¼˜åŒ–å¥–åŠ±/æ¨¡å‹")
    else:
        print("âŒ è¾ƒå·®ï¼šæˆåŠŸç‡<50%ï¼Œéœ€é‡æ„å¥–åŠ±ç³»ç»Ÿ")

    env.close()
    return {
        "success_rate": success_rate,
        "avg_steps": avg_steps,
        "avg_reward": avg_reward
    }

# ====================== 6. è®­ç»ƒ+éªŒè¯ä¸€ä½“åŒ– ======================
def train_and_validate(train_episodes=500, val_episodes=100, save_model_path="dqn_maze_model.pth"):
    """å…ˆè®­ç»ƒï¼Œå†éªŒè¯ï¼Œå¹¶ä¿å­˜æ¨¡å‹"""
    # 1. è®­ç»ƒæ™ºèƒ½ä½“
    agent = DQNAgent()
    agent.train(episodes=train_episodes)
    
    # 2. ä¿å­˜æ¨¡å‹
    torch.save(agent.policy_net.state_dict(), save_model_path)
    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{save_model_path}")
    
    # 3. éªŒè¯æ¨¡å‹
    validate_agent(model_path=save_model_path, episodes=val_episodes, render=True)

# ====================== æ‰§è¡Œå…¥å£ ======================
if __name__ == "__main__":
    # è‡ªåŠ¨å®‰è£…ä¾èµ–
    try:
        import pygame
    except ImportError:
        print("ğŸ“¦ æ­£åœ¨å®‰è£…pygame...")
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "pygame"])
        import pygame

    # é€‰æ‹©æ‰§è¡Œæ–¹å¼ï¼š
    # æ–¹å¼1ï¼šä»…è®­ç»ƒ
    # agent = DQNAgent()
    # agent.train(episodes=500)

    # æ–¹å¼2ï¼šä»…éªŒè¯ï¼ˆéœ€å…ˆè®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ï¼‰
    #validate_agent(model_path="dqn_maze_model.pth", episodes=100, render=True)

    # æ–¹å¼3ï¼šè®­ç»ƒ+éªŒè¯ï¼ˆæ¨èï¼‰
    train_and_validate(train_episodes=500, val_episodes=100)